{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9eab9c3",
   "metadata": {},
   "source": [
    "# 🎓 **Inteligencia Artificial Aplicada**\n",
    "\n",
    "## 🤖 **Operaciones de aprendizaje automático (Gpo 10)**\n",
    "\n",
    "### 🏛️ Tecnológico de Monterrey\n",
    "\n",
    "#### 👨‍🏫 **Profesor titular :** Dr. Gerardo Rodríguez Hernández\n",
    "#### 👩‍🏫 **Profesor titular :** Maestro Ricardo Valdez Hernández\n",
    "#### 👩‍🏫 **Profesor tutor :** Jorge Gonzales Zapata\n",
    "\n",
    "### 📊 **Fase 1 Proyecto MLOps**\n",
    "\n",
    "#### 📅 **Octubre de 2025**\n",
    "\n",
    "### 👥 Equipo 43\n",
    "\n",
    "* 🧑‍💻 **A01795645 :** Alberto Campos Hernández\n",
    "* 🧑‍💻 **A01016093 :** Oscar Enrique García García\n",
    "* 🧑‍💻 **A01795922 :** Jessica Giovana García Gómez\n",
    "* 🧑‍💻 **A01795897 :** Esteban Sebastián Guerra Espinoza\n",
    "* 🧑‍💻 **A00820345 :** Rafael Sánchez Marmolejo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad88d8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Optional, List\n",
    "from pathlib import Path\n",
    "# Configuración global de paralelismo para todos los modelos\n",
    "N_JOBS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a3a0b85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# CARGA DE ARCHIVOS\n",
    "# ==========================\n",
    "@dataclass\n",
    "class CargaArchivos:\n",
    "    \"\"\"Carga datasets crudos desde una carpeta.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    carpeta_raw: str | Path\n",
    "        Ruta a la carpeta que contiene los CSVs crudos.\n",
    "    nombre_modificado: str\n",
    "        Nombre del CSV \"modificado\". power_tetouan_city_modified.csv\n",
    "    \"\"\"\n",
    "\n",
    "    carpeta_raw: Path\n",
    "    nombre_modificado: str\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.carpeta_raw = Path(self.carpeta_raw)\n",
    "        self.carpeta_raw.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    def leer(self) -> pd.DataFrame:\n",
    "        na_vals = [\"nan\", \"NAN\", \"NaT\", \"\"]\n",
    "        df_modificado = pd.read_csv(\n",
    "            self.carpeta_raw / self.nombre_modificado,\n",
    "            na_values=na_vals,\n",
    "            keep_default_na=True,\n",
    "        )\n",
    "        return df_modificado\n",
    "\n",
    "\n",
    "# ==========================\n",
    "# PREPROCESAMIENTO\n",
    "# ==========================\n",
    "@dataclass\n",
    "class Preprocesamiento:\n",
    "    \"\"\"Transforma el dataset modificado en un dataset listo para modelar.\n",
    "    Pasos realizados:\n",
    "    - Elimina columna \"mixed_type_col\" si existe.\n",
    "    - Limpia y convierte DateTime con distintos formatos.\n",
    "    - Imputa DateTime faltante con vecino a ±10 min o punto medio.\n",
    "    - Imputa numéricos con mediana por columna.\n",
    "    - Maneja outliers mediante IQR + mediana rodante (ventana configurable).\n",
    "    - Crea variables de tiempo y elimina DateTime si se solicita.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _tranformar_numerica(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        cols = df.columns[1:9]\n",
    "        df[cols] = (\n",
    "            df[cols]\n",
    "            .astype(str)\n",
    "            .apply(lambda s: s.str.replace(',', '.', regex=False).str.strip())\n",
    "            .apply(pd.to_numeric, errors='coerce')\n",
    "        )\n",
    "        df[cols].dtypes\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _drop_col_si_existe(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "        return df.drop(columns=[col], errors=\"ignore\")\n",
    "\n",
    "    @staticmethod\n",
    "    def _limpiar_parsear_datetime(df: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "        s = (\n",
    "            df[col].astype(str)\n",
    "            .str.replace(r\"[\\r\\n\\t]+\", \" \", regex=True)\n",
    "            .str.strip()\n",
    "        )\n",
    "        s = s.mask(s.eq(\"\"))\n",
    "        s = s.mask(s.str.lower().eq(\"nan\"))\n",
    "\n",
    "        dt = pd.to_datetime(s, errors=\"coerce\")\n",
    "        miss = dt.isna()\n",
    "        # Segundo intento con formato explícito mm/dd/YYYY HH:MM\n",
    "        dt.loc[miss] = pd.to_datetime(\n",
    "            s[miss], format=\"%m/%d/%Y %H:%M\", errors=\"coerce\"\n",
    "        )\n",
    "\n",
    "        # Imputación por vecinos: 10 minutos o punto medio\n",
    "        prev = dt.shift(1)\n",
    "        nxt = dt.shift(-1)\n",
    "        mask = dt.isna() & prev.notna() & nxt.notna()\n",
    "\n",
    "        m10 = mask & ((nxt - prev) == pd.Timedelta(minutes=20))\n",
    "        dt.loc[m10] = prev.loc[m10] + pd.Timedelta(minutes=10)\n",
    "\n",
    "        m_mid = mask & dt.isna()\n",
    "        if m_mid.any():\n",
    "            mid_ns = (prev[m_mid].astype(\"int64\") + nxt[m_mid].astype(\"int64\")) // 2\n",
    "            dt.loc[m_mid] = pd.to_datetime(mid_ns)\n",
    "\n",
    "        df[col] = dt\n",
    "\n",
    "        valid = df['DateTime'].notna() & ~df['DateTime'].astype(str).str.strip().str.lower().eq('nan')\n",
    "        df['__score__'] = df.drop(columns=['DateTime']).notna().sum(axis=1)\n",
    "        keep = (df.loc[valid]\n",
    "                .sort_values(['DateTime','__score__'], ascending=[True, False])\n",
    "                .drop_duplicates(subset=['DateTime'], keep='first'))\n",
    "\n",
    "        df= (pd.concat([keep, df.loc[~valid]])\n",
    "                                    .drop(columns='__score__')\n",
    "                                    .sort_index()\n",
    "                                    .reset_index(drop=True))\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def _imputar_numericos_mediana(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        num_cols = df.select_dtypes(include=\"number\").columns\n",
    "        medianas = df[num_cols].median()\n",
    "        df[num_cols] = df[num_cols].fillna(medianas)\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def _outliers_mediana_rodante(df: pd.DataFrame, col_fecha: str, ventana_mediana: int) -> pd.DataFrame:\n",
    "        df = df.sort_values(col_fecha).copy()\n",
    "        num = df.select_dtypes(\"number\").columns\n",
    "\n",
    "        Q1, Q3 = df[num].quantile(0.25), df[num].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lo, hi = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "        mask = (df[num] < lo) | (df[num] > hi)\n",
    "\n",
    "        for c in num:\n",
    "            rmed = df[c].rolling(window=ventana_mediana, center=True, min_periods=1).median()\n",
    "            df.loc[mask[c], c] = rmed[mask[c]].fillna(df[c].median())\n",
    "        \n",
    "        df=df.dropna()\n",
    "        return df\n",
    "    \n",
    "    @staticmethod\n",
    "    def _features_tiempo(df: pd.DataFrame, col_fecha: str) -> pd.DataFrame:\n",
    "        dt = df[col_fecha]\n",
    "        df[\"Day\"] = dt.dt.day\n",
    "        df[\"Month\"] = dt.dt.month\n",
    "        df[\"Hour\"] = dt.dt.hour\n",
    "        df[\"Minute\"] = dt.dt.minute\n",
    "        df[\"Day of Week\"] = dt.dt.dayofweek + 1\n",
    "        # Quarter\n",
    "        df[\"Quarter of Year\"] = pd.cut(\n",
    "            df[\"Month\"],\n",
    "            bins=[0, 3, 6, 9, 12],\n",
    "            labels=[1, 2, 3, 4],\n",
    "            include_lowest=True,\n",
    "        ).astype(int)\n",
    "        # Day of Year\n",
    "        df[\"Day of Year\"] = dt.dt.strftime('%j').astype(int)\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def _finalizar(df: pd.DataFrame, col_fecha: str, eliminar_datetime: bool) -> pd.DataFrame:\n",
    "        df = df.dropna().copy()\n",
    "        if eliminar_datetime and col_fecha in df.columns:\n",
    "            df = df.drop(columns=[col_fecha])\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def ejecutar(df_modificado: pd.DataFrame, *, ventana_mediana: int, eliminar_datetime: bool) -> pd.DataFrame:\n",
    "        df = df_modificado.copy()\n",
    "        df = Preprocesamiento._tranformar_numerica(df)\n",
    "        df = Preprocesamiento._drop_col_si_existe(df, \"mixed_type_col\")\n",
    "        df = Preprocesamiento._limpiar_parsear_datetime(df, \"DateTime\")\n",
    "        df = Preprocesamiento._imputar_numericos_mediana(df)\n",
    "        df = Preprocesamiento._outliers_mediana_rodante(df, \"DateTime\", ventana_mediana)\n",
    "        df = Preprocesamiento._features_tiempo(df, \"DateTime\")\n",
    "        df = Preprocesamiento._finalizar(df, \"DateTime\", eliminar_datetime)\n",
    "        return df\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def correr_pipeline(\n",
    "        carpeta_raw: str | Path = \"../data/raw\",\n",
    "        carpeta_processed: str | Path = \"../data/processed\",\n",
    "        nombre_salida: str = \"power_tetouan_city_processed1.csv\",\n",
    "        nombre_modificado: str = \"power_tetouan_city_modified.csv\",\n",
    "        ventana_mediana: int = 25,\n",
    "        eliminar_datetime: bool = True,\n",
    "    ) -> Path:\n",
    "        carpeta_processed = Path(carpeta_processed)\n",
    "        carpeta_processed.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        loader = CargaArchivos(carpeta_raw, nombre_modificado)\n",
    "        df_modificado = loader.leer()\n",
    "\n",
    "        df_final = Preprocesamiento.ejecutar(df_modificado,ventana_mediana=ventana_mediana,eliminar_datetime=eliminar_datetime,)\n",
    "\n",
    "        ruta_out = carpeta_processed / nombre_salida\n",
    "        df_final.to_csv(ruta_out, index=False)\n",
    "        return ruta_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdb3db0",
   "metadata": {},
   "source": [
    "Prueba de las clases Carga y Procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "53425d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Prepara un CSV de ejemplo\n",
    "carpeta = Path(\"../data/raw\")\n",
    "\n",
    "# 2) Instancia y lee\n",
    "carga = CargaArchivos(\n",
    "    carpeta_raw=carpeta,\n",
    "    nombre_modificado=\"power_tetouan_city_modified.csv\",\n",
    ")\n",
    "\n",
    "df = carga.leer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "564a7ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WindowsPath('../data/processed/power_tetouan_city_processed.csv')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Preprocesamiento.correr_pipeline( \"../data/raw\", \"../data/processed\", \"power_tetouan_city_processed.csv\",\"power_tetouan_city_modified.csv\",25,True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a20f135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c15b7443",
   "metadata": {},
   "source": [
    "Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1577c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedKFold, cross_val_score\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Núcleos paralelos por defecto\n",
    "N_JOBS = -1\n",
    "\n",
    "# XGBoost opcional\n",
    "try:\n",
    "    from xgboost import XGBRegressor\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "class Train:\n",
    "    def __init__(\n",
    "        self,\n",
    "        df: pd.DataFrame,\n",
    "        target: str = \"PowerConsumption_Zone2\",\n",
    "        num_cols = ('Temperature','Humidity','WindSpeed','GeneralDiffuseFlows','DiffuseFlows'),\n",
    "        feature_range=(1,2),\n",
    "        train_ratio: float = 0.80,\n",
    "        random_state: int = 42\n",
    "    ):\n",
    "        self.df = df.copy()\n",
    "        self.target = target\n",
    "        self.num_cols = list(num_cols)\n",
    "        self.feature_range = feature_range\n",
    "        self.train_ratio = train_ratio\n",
    "        self.random_state = random_state\n",
    "\n",
    "        self.df.columns=['Temperature', 'Humidity', 'WindSpeed', 'GeneralDiffuseFlows',\n",
    "       'DiffuseFlows','PowerConsumption_Zone1',\n",
    "       'PowerConsumption_Zone2', 'PowerConsumption_Zone3' ,'Day',\n",
    "       'Month', 'Hour', 'Minute', 'DayWeek', 'QuarterYear',\n",
    "       'DayYear']\n",
    "\n",
    "        # split temporal como en tu código\n",
    "        n = len(self.df)\n",
    "        i = int(n * self.train_ratio)\n",
    "\n",
    "        self.X = self.df.drop(columns=['PowerConsumption_Zone1','PowerConsumption_Zone2','PowerConsumption_Zone3'])\n",
    "        self.y = self.df[[self.target]]\n",
    "\n",
    "        self.x_train, self.y_train = self.X.iloc[:i], self.y.iloc[:i].values.ravel()\n",
    "        self.x_test,  self.y_test  = self.X.iloc[i:],  self.y.iloc[i:].values.ravel()\n",
    "\n",
    "        # preprocesamiento\n",
    "        self.num_pipeline = Pipeline(steps=[\n",
    "            ('impMediana', SimpleImputer(strategy='median')),\n",
    "            ('escalaNum', MinMaxScaler(feature_range=self.feature_range)),\n",
    "        ])\n",
    "        self.ct = ColumnTransformer(\n",
    "            transformers=[('numpipe', self.num_pipeline, self.num_cols)],\n",
    "            remainder='passthrough'\n",
    "        )\n",
    "\n",
    "        # modelos\n",
    "        self.modelos, self.nombres = self._mis_modelos()\n",
    "\n",
    "        # salidas\n",
    "        self.cv_results_ = None\n",
    "        self.best_name_ = None\n",
    "        self.best_estimator_ = None          # modelo base\n",
    "        self.best_pipeline_ = None           # pipeline(ct + modelo) entrenado en train\n",
    "        self.test_rmse_ = None\n",
    "\n",
    "    def _mis_modelos(self):\n",
    "        modelos, nombres = [], []\n",
    "\n",
    "        modelos.append(RandomForestRegressor(\n",
    "            n_estimators=700, min_samples_split=2, min_samples_leaf=1,\n",
    "            max_features=3, random_state=self.random_state, n_jobs=N_JOBS\n",
    "        )); nombres.append('RandomForest')\n",
    "\n",
    "        modelos.append(ElasticNet(\n",
    "            alpha=0.1, l1_ratio=0.5, random_state=self.random_state, max_iter=5000\n",
    "        )); nombres.append('ElasticNet')\n",
    "\n",
    "        modelos.append(GradientBoostingRegressor(\n",
    "            n_estimators=500, learning_rate=0.05, max_depth=5,\n",
    "            min_samples_split=5, min_samples_leaf=3, random_state=self.random_state\n",
    "        )); nombres.append('GradientBoosting')\n",
    "\n",
    "        if XGBOOST_AVAILABLE:\n",
    "            modelos.append(XGBRegressor(\n",
    "                n_estimators=500, learning_rate=0.05, max_depth=5,\n",
    "                random_state=self.random_state, n_jobs=N_JOBS\n",
    "            )); nombres.append('XGBoost')\n",
    "\n",
    "        modelos.append(SVR(kernel='rbf', C=100, epsilon=0.1, gamma='scale'))\n",
    "        nombres.append('SVR')\n",
    "\n",
    "        return modelos, nombres\n",
    "\n",
    "    def cross_validate(self, n_splits=5, n_repeats=2, scoring='neg_mean_squared_error'):\n",
    "        cv = RepeatedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=8)\n",
    "        filas = []\n",
    "        detalles = {}\n",
    "\n",
    "        for modelo, nombre in zip(self.modelos, self.nombres):\n",
    "            pipe = Pipeline(steps=[('ct', self.ct), ('m', modelo)])\n",
    "            mse_scores = cross_val_score(\n",
    "                pipe, self.x_train, self.y_train,\n",
    "                scoring=scoring, cv=cv, n_jobs=N_JOBS\n",
    "            )\n",
    "            rmse = np.sqrt(-mse_scores)\n",
    "            filas.append({'model': nombre, 'rmse_mean': rmse.mean(), 'rmse_std': rmse.std()})\n",
    "            detalles[nombre] = rmse\n",
    "\n",
    "        self.cv_results_ = pd.DataFrame(filas).sort_values('rmse_mean').reset_index(drop=True)\n",
    "        return self.cv_results_, detalles\n",
    "\n",
    "    def fit_best(self):\n",
    "        if self.cv_results_ is None or self.cv_results_.empty:\n",
    "            self.cross_validate()\n",
    "\n",
    "        self.best_name_ = self.cv_results_.iloc[0]['model']\n",
    "        idx = self.nombres.index(self.best_name_)\n",
    "        self.best_estimator_ = self.modelos[idx]\n",
    "\n",
    "        self.best_pipeline_ = Pipeline(steps=[('ct', self.ct), ('m', self.best_estimator_)])\n",
    "        self.best_pipeline_.fit(self.x_train, self.y_train)\n",
    "\n",
    "        preds = self.best_pipeline_.predict(self.x_test)\n",
    "        self.test_rmse_ = float(np.sqrt(mean_squared_error(self.y_test, preds)))\n",
    "        return self.best_pipeline_, self.test_rmse_\n",
    "\n",
    "    def predict(self, X_new: pd.DataFrame):\n",
    "        if self.best_pipeline_ is None:\n",
    "            raise RuntimeError(\"Primero ejecuta fit_best().\")\n",
    "        return self.best_pipeline_.predict(X_new)\n",
    "\n",
    "    def get_best(self):\n",
    "        if self.best_pipeline_ is None:\n",
    "            raise RuntimeError(\"Aún no hay modelo entrenado. Llama a fit_best().\")\n",
    "        return {\n",
    "            'name': self.best_name_,\n",
    "            'pipeline': self.best_pipeline_,\n",
    "            'test_rmse': self.test_rmse_,\n",
    "            'cv_table': self.cv_results_.copy()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "063b9cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              model    rmse_mean   rmse_std\n",
      "0      RandomForest   863.924245  35.264463\n",
      "1           XGBoost   951.048584  32.452242\n",
      "2  GradientBoosting   961.329366  32.197826\n",
      "3        ElasticNet  3307.388993  27.333673\n",
      "4               SVR  3374.946358  28.530995\n",
      "Mejor: RandomForest RMSE test: 3700.542\n"
     ]
    }
   ],
   "source": [
    "# 1) Prepara un CSV de ejemplo\n",
    "carpeta = Path(\"../data/processed\")\n",
    "\n",
    "# 2) Instancia y lee\n",
    "carga = CargaArchivos(\n",
    "    carpeta_raw=carpeta,\n",
    "    nombre_modificado=\"power_tetouan_city_processed.csv\",\n",
    ")\n",
    "\n",
    "df = carga.leer()\n",
    "train = Train(df)\n",
    "cv_table, _ = train.cross_validate()\n",
    "best_pipe, test_rmse = train.fit_best()\n",
    "print(cv_table)\n",
    "print(\"Mejor:\", train.best_name_, \"RMSE test:\", round(test_rmse, 3))\n",
    "y_pred = train.predict(train.x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
